{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon_video_games_sentiment.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBvdJ3_DAtbd",
        "colab_type": "text"
      },
      "source": [
        "# Amazon Video Games Sentiment Analysis\n",
        "Raj Prasad\n",
        "July 2019\n",
        "\n",
        "[html version](https://daddyprasad5.github.io/amazon_video_games_sentiment.html) - with all the code hidden away for a quick read\n",
        "\n",
        "[jupyter notebook version](https://github.com/daddyprasad5/thinkful/blob/amazon_video_games_sentiment.ipynb) - with all the code exposed in an interactive notebook\n",
        "\n",
        "The goal of this exercise is to exercise spark batch processing skills in a machine learning context.  \n",
        "\n",
        "I'll be building a sentiment analysis model using TF/IDF features and a logistic regression model.  \n",
        "\n",
        "Much thanks to [Ricky Kim](https://towardsdatascience.com/@rickykim78?source=post_page---------------------------) whose [Medium Post ](https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35) on this topic was extremely useful and easy to follow. \n",
        "\n",
        "This notebook is designed to work on google colab. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWlZKZq_Cu8g",
        "colab_type": "text"
      },
      "source": [
        "Here are the basic gottadoit's for any spark-on-colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN3XGZ9Fof5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "dc2bddbc-1c6a-412c-e5b4-7ad43d549408"
      },
      "source": [
        "#install spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.3-bin-hadoop2.7.tgz\n",
        "\n",
        "# Install spark-related depdencies for Python\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "# Set up required environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\"\n",
        "\n",
        "# Point Colaboratory to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n",
            "\u001b[K     |████████████████████████████████| 215.6MB 47kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 40.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.3\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq7RM6W7o0Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdQTNYq9o3v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set constants\n",
        "JSON_PATH = \"/content/gdrive/My Drive/Colab Datasets/reviews_Video_Games_5.json\" \n",
        "APP_NAME = \"Amazon Video Game Sentiment Analysis\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141107\n",
        "TRAINING_DATA_RATIO = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkO4LZv6rluM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read our data\n",
        "#data downloaded from: http://jmcauley.ucsd.edu/data/amazon/\n",
        "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
        "df = spark.read.options(inferschema = \"true\").json(JSON_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDHGvJ3AsegS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I need only the \"asin\" (id), \"overall\" (rating), and \"reviewText\" columns.  \n",
        "reviews = df.select([\"asin\", \"overall\", \"reviewText\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtZ_ObvMtWGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Translate the 1-5 \"overall\" score to negative (0) or positive (1) sentiment\n",
        "reviews = reviews.withColumn(\"target\", \n",
        "                             when(reviews.overall <= 3, 0).otherwise(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN4GVoLbxVL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splite into training, validation and test\n",
        "(train_set, val_set, test_set) = reviews.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPwzrp2lDX7S",
        "colab_type": "text"
      },
      "source": [
        "I've done three different versions of the model - following Ricky's lead.  \n",
        "\n",
        "*   TD/IDF with single word terms, using the HashingTF IDF feature calculator\n",
        "*   TD/IDF with single word terms, using the CountVectorizer feature calculator\n",
        "*  TD/IDF with n (1-3)-gram terms, using the CountVectorizer feature calculator\n",
        "\n",
        "You'll see that the middle version was the best balance of performance and speed in this case.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG-rytmbx5VT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "309f9fa1-1de2-4146-aafd-0b2246a6317d"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
        "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
        "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "train_df = pipelineFit.transform(train_set)\n",
        "val_df = pipelineFit.transform(val_set)\n",
        "train_df.show(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
            "|      asin|overall|          reviewText|target|               words|                  tf|            features|label|\n",
            "+----------+-------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
            "|0700099867|    1.0|1st shipment rece...|     0|[1st, shipment, r...|(65536,[568,6534,...|(65536,[568,6534,...|  1.0|\n",
            "|0700099867|    1.0|Crashed in Vista....|     0|[crashed, in, vis...|(65536,[4775,8315...|(65536,[4775,8315...|  1.0|\n",
            "|0700099867|    1.0|DiRT 2 was like t...|     0|[dirt, 2, was, li...|(65536,[1672,1706...|(65536,[1672,1706...|  1.0|\n",
            "|0700099867|    1.0|I bought this and...|     0|[i, bought, this,...|(65536,[5782,8436...|(65536,[5782,8436...|  1.0|\n",
            "|0700099867|    1.0|I can't tell you ...|     0|[i, can't, tell, ...|(65536,[1903,2026...|(65536,[1903,2026...|  1.0|\n",
            "+----------+-------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x4tM00hzbTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "683f59a5-6843-403a-e088-877ed261c360"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(maxIter=100)\n",
        "lrModel = lr.fit(train_df)\n",
        "predictions = lrModel.transform(val_df)\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "#areaUnderROC\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7606488356568426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7T7DIG2zzhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ff1e856-5834-4b6e-d584-873cb1747d47"
      },
      "source": [
        "#accuracy\n",
        "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
        "accuracy"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7815013404825737"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tpA30z-0blX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dc2b3829-35f9-4870-f254-b1ee78f1ec5c"
      },
      "source": [
        "#same process, but using a different inverse document frequency calculator\n",
        "\n",
        "%%time\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
        "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
        "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "lr = LogisticRegression(maxIter=100)\n",
        "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
        "\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "predictions = pipelineFit.transform(val_set)\n",
        "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
        "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.7958\n",
            "ROC-AUC: 0.8107\n",
            "CPU times: user 135 ms, sys: 23.6 ms, total: 158 ms\n",
            "Wall time: 3min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy4uFYpq3Xru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#same as above, but using n-grams (n=1-3) instead of single words\n",
        "\n",
        "from pyspark.ml.feature import NGram, VectorAssembler\n",
        "\n",
        "def build_ngrams_wocs(inputCol=[\"reviewText\",\"target\"], n=3):\n",
        "    tokenizer = [Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")]\n",
        "    ngrams = [\n",
        "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "\n",
        "    cv = [\n",
        "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
        "            outputCol=\"{0}_tf\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), \n",
        "               minDocFreq=5) for i in range(1, n + 1)]\n",
        "\n",
        "    assembler = [VectorAssembler(\n",
        "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
        "        outputCol=\"features\"\n",
        "    )]\n",
        "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
        "    lr = [LogisticRegression(maxIter=100)]\n",
        "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler \n",
        "                    + label_stringIdx+lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azDof30g5J2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26a1622f-8033-4e71-c780-75f0c2066188"
      },
      "source": [
        "%%time\n",
        "trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)\n",
        "predictions_wocs = trigramwocs_pipelineFit.transform(val_set)\n",
        "accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float(val_set.count())\n",
        "roc_auc_wocs = evaluator.evaluate(predictions_wocs)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 519 ms, sys: 121 ms, total: 640 ms\n",
            "Wall time: 25min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tae2sZxyC8wj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ce6646f9-a2f4-4d5a-b747-6aed5bc0f18b"
      },
      "source": [
        "# print accuracy, roc_auc\n",
        "print(\"Accuracy Score: {0:.4f}\".format(accuracy_wocs))\n",
        "print(\"ROC-AUC: {0:.4f}\".format(roc_auc_wocs))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.8552\n",
            "ROC-AUC: 0.8917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4dpQAOsAEht",
        "colab_type": "text"
      },
      "source": [
        "That improved things somewhat.  Normally, I'd move on to tuning, but with 25 minutes per run, I'll leave the code below un-executed.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZXUx5yv5CFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create ParamGrid for Cross Validation\n",
        "\n",
        "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# paramGrid = (ParamGridBuilder()\n",
        "#              .addGrid(lr.regParam, [0.01, 2.0])\n",
        "#              .addGrid(lr.elasticNetParam, [0.0, 1.0])\n",
        "#              .addGrid(lr.maxIter, [1, 10])\n",
        "#              .build())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TngtzYxHZ4ob",
        "colab_type": "text"
      },
      "source": [
        "# EVERYTHING BELOW HERE IS NOT RUN & TESTED\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKUGgbiwFX1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create 5-fold CrossValidator\n",
        "# cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
        "\n",
        "# # Run cross validations\n",
        "# cvModel = cv.fit(train_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZPlon0tHKbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use test set to measure the accuracy of our model on new data\n",
        "# predictions = cvModel.transform(val_set)\n",
        "\n",
        "# # cvModel uses the best model found from the Cross Validation\n",
        "# # Evaluate best model\n",
        "# evaluator.evaluate(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF7eyfeRHWBg",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's run the test set through the best model.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrXIAljU5tLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bfbef734-64dd-4dd2-88a1-688a927cd092"
      },
      "source": [
        "\n",
        "%%time\n",
        "predictions_wocs_test = trigramwocs_pipelineFit.transform(test_set)\n",
        "accuracy_wocs_test = predictions_wocs_test.filter(predictions_wocs_test.label == predictions_wocs_test.prediction).count() / float(val_set.count())\n",
        "roc_auc_wocs_test = evaluator.evaluate(predictions_wocs_test)\n",
        "\n",
        "\n",
        "# print accuracy, roc_auc\n",
        "print(\"Accuracy Score: {0:.4f}\".format(accuracy_wocs_test))\n",
        "print(\"ROC-AUC: {0:.4f}\".format(roc_auc_wocs_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.8878\n",
            "ROC-AUC: 0.8697\n",
            "CPU times: user 61.5 ms, sys: 17.8 ms, total: 79.3 ms\n",
            "Wall time: 16.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}